{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosas que se asumen\n\n1. **Ingesta de Datos**:\n   - La data fue previamente ingestada desde Google Drive hacia Azure Data Lake. Esto se debe a que el antivirus de Google Drive impide usar un conector o importar mediante un trigger la data. En cualquier otro caso, se podría conectar a una fuente externa como una base de datos o mediante algún API.\n\n2. **Procesamiento por Lotes (Batch)**:\n   - El procesamiento se realiza en lotes (batch) y la data se consulta en la ruta del datalake:\n     ```markdown\n     https://rawlatamdata.blob.core.windows.net/rawdata/tweets.json.zip\n     ```\n     Luego, se descomprime en formato JSON en:\n     ```markdown\n     https://rawlatamdata.blob.core.windows.net/processeddata/farmers-protest-tweets-2021-2-4.json\n     ```\n\n3. **Ejecución Manual del Pipeline**:\n   - El pipeline de ingesta se ejecuta manualmente, leyendo el archivo almacenado en el Data Lake. Asumiendo que esta parte fuese automatizada, se podría establecer un trigger periódico para traer la data desde una fuente externa con cada ejecución. Sin embargo, el ejercicio actual no lo requiere.\n\n4. **Cluster Interactivo para Procesamiento**:\n   - Para el procesamiento de la información y los cálculos, se utilizó Databricks con un clúster interactivo. Este clúster puede ser reemplazado por un job cluster o un pool de instancias para reducir costos. No obstante, para este ejercicio, se utiliza un clúster interactivo por facilidad de uso y demostración.\n\n   **Configuración del Clúster**:\n   - **Cluster ID**: `0619-233222-rjlnjfll`\n   - **Usuario Creador**: `juan.afanador24@hotmail.com`\n   - **Tipo de Nodo**: `Standard_DS3_v2`\n   - **Versión de Spark**: `15.2.x-scala2.12`\n   - **Máximo de Núcleos**: `4`\n   - **Memoria Total**: `14.3 GB`\n   - **Tiempo de Autoterminado**: `30 minutos`\n   - **Tipo de Disponibilidad**: `ON_DEMAND_AZURE`\n\n5. **Seguridad y Almacenamiento de Credenciales**:\n   - Para proteger credenciales y mantener la consistencia de las rutas, se optó por almacenar datos sensibles en Azure Key Vault junto con Unity Catalog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

