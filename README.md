<!-- Logo de LATAM -->
<p align="center">
  <img src="https://pressreleasecom.wordpress.com/wp-content/uploads/2017/06/pressrelease-logo-latam-airlines.jpg" width="300">
</p>

<!-- T칤tulo del Proyecto -->
<h1 align="center" style="color: #A01F30;">Documentaci칩n del Proyecto de An치lisis de Tweets</h1>

<!-- Descripci칩n General -->
<p align="center" style="color: #003366; font-size: 18px;">
  Bienvenido a la documentaci칩n del proyecto de an치lisis de tweets. Este proyecto utiliza PySpark para procesar y analizar un archivo JSON de tweets almacenado en Azure Data Lake Storage (ADLS).
</p>

<!-- Detalles del Proyecto -->
## Descripci칩n del Proyecto

Este proyecto se enfoca en el an치lisis de datos de tweets para obtener insights valiosos. Los principales objetivos del proyecto son:

1. 游늰 Identificar las 10 fechas con la mayor cantidad de tweets y el usuario m치s activo en cada una de esas fechas.
2. 游땕 Encontrar los 10 emojis m치s usados y su respectivo conteo.
3. 游끥 Determinar los 10 usuarios m치s influyentes en funci칩n del conteo de menciones (@).

## Pasos del Proceso

### 1. Inicializaci칩n y Configuraci칩n
- Definir las rutas base para los archivos de datos, de salida y de logs.
- Crear los directorios de salida y logs si no existen.
- Configurar un logger personalizado para registrar mensajes en un archivo de texto.

### 2. Ejecuci칩n del An치lisis
Cada uno de los an치lisis se realiza a trav칠s de funciones espec칤ficas, optimizadas para el uso eficiente del tiempo y la memoria.

- **[游늰 An치lisis de Fechas y Usuarios Activos](docs/q1_memory.md)**
- **[游땕 An치lisis de Emojis Usados](docs/q2_memory.md)**
- **[游끥 An치lisis de Usuarios Influyentes](docs/q3_memory.md)**

### 3. Resultados y Logs
- Los resultados se guardan en formato Delta en ADLS.
- Los detalles de la ejecuci칩n se registran en un archivo de log para referencia futura.

### Cosas que se asumen

1. **Ingesta de Datos**:
   - La data fue previamente ingestada desde Google Drive hacia Azure Data Lake. Esto se debe a que el antivirus de Google Drive impide usar un conector o importar mediante un trigger la data. En cualquier otro caso, se podr칤a conectar a una fuente externa como una base de datos o mediante alg칰n API.

2. **Procesamiento por Lotes (Batch)**:
   - El procesamiento se realiza en lotes (batch) y la data se consulta en la ruta del datalake:
     ```markdown
     https://rawlatamdata.blob.core.windows.net/rawdata/tweets.json.zip
     ```
     Luego, se descomprime en formato JSON en:
     ```markdown
     https://rawlatamdata.blob.core.windows.net/processeddata/farmers-protest-tweets-2021-2-4.json
     ```

3. **Ejecuci칩n Manual del Pipeline**:
   - El pipeline de ingesta se ejecuta manualmente, leyendo el archivo almacenado en el Data Lake. Asumiendo que esta parte fuese automatizada, se podr칤a establecer un trigger peri칩dico para traer la data desde una fuente externa con cada ejecuci칩n. Sin embargo, el ejercicio actual no lo requiere.

4. **Cluster Interactivo para Procesamiento**:
   - Para el procesamiento de la informaci칩n y los c치lculos, se utiliz칩 Databricks con un cl칰ster interactivo. Este cl칰ster puede ser reemplazado por un job cluster o un pool de instancias para reducir costos. No obstante, para este ejercicio, se utiliza un cl칰ster interactivo por facilidad de uso y demostraci칩n.

   **Configuraci칩n del Cl칰ster**:
   - **Cluster ID**: `0619-233222-rjlnjfll`
   - **Usuario Creador**: `juan.afanador24@hotmail.com`
   - **Tipo de Nodo**: `Standard_DS3_v2`
   - **Versi칩n de Spark**: `15.2.x-scala2.12`
   - **M치ximo de N칰cleos**: `4`
   - **Memoria Total**: `14.3 GB`
   - **Tiempo de Autoterminado**: `30 minutos`
   - **Tipo de Disponibilidad**: `ON_DEMAND_AZURE`

5. **Seguridad y Almacenamiento de Credenciales**:
   - Para proteger credenciales y mantener la consistencia de las rutas, se opt칩 por almacenar datos sensibles en Azure Key Vault junto con Unity Catalog.

## Arquitectura

A continuaci칩n se presenta la arquitectura del proyecto, ilustrada en la siguiente imagen:

![Arquitectura del Proyecto](https://imgur.com/YbUe0le.png)

### Descripci칩n de la Arquitectura

La arquitectura del proyecto se compone de varios componentes clave:

1. **Azure Data Factory**:
   - Orquesta y automatiza los flujos de datos.
   - Ejecuta los pipelines de ingesta y procesamiento.

2. **Azure Data Lake Storage**:
   - Almacena los datos en formato crudo (raw) y procesado (processed).
   - Facilita el almacenamiento y acceso eficiente de grandes vol칰menes de datos.

3. **Azure Databricks**:
   - Procesa los datos utilizando notebooks en PySpark.
   - Ejecuta los c치lculos y an치lisis de los datos.

4. **Azure Key Vault**:
   - Gestiona y protege las credenciales y secretos utilizados en el proyecto.
   - Asegura el acceso seguro a los recursos.

5. **Pipelines de Azure Data Factory**:
   - **Raw_ingestion**: Ingresa los datos crudos desde la fuente.
   - **Refined_ingestion**: Procesa y transforma los datos para an치lisis.

6. **Resultados**:
   - Los resultados del procesamiento se almacenan en formato Delta en el Data Lake.
   - Los resultados pueden consultarse directamente desde los notebooks o desde los archivos Delta.
7. **Logs**:
   - Los resultados del procesamiento en metricas de tiempo de ejecuci칩n y memoria se guardan junto con cada ejecuci칩n en la carpeta logs como un archivo .txt


## Video Explicativo

En este video, se muestra un tutorial paso a paso del proceso de orquestaci칩n y procesamiento de datos.

[![Watch the video](https://img.youtube.com/vi/kcI-SuW4mMY/0.jpg)](https://www.youtube.com/watch?v=kcI-SuW4mMY)

## Modularidad y Parametrizaci칩n

El proyecto est치 dise침ado de manera modular y parametrizada utilizando Azure Data Factory. Esto significa que cualquier cambio en la fuente de ingesta, por ejemplo, puede realizarse f치cilmente desde una variable en la f치brica de datos. Esta configuraci칩n permite una gran flexibilidad y adaptabilidad a diferentes fuentes de datos sin necesidad de modificar el c칩digo.

## Resultados

Los resultados del procesamiento pueden ser consultados en el print de cada notebook o revisando el archivo Delta guardado en la salida del Data Lake. Para facilitar la consulta, los resultados se listan a continuaci칩n:

1. **q1**:
   - [(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]

2. **q2**:
   - [('游똂', 7286), ('游땍', 3072), ('游뚶', 2972), ('游', 2363), ('游', 2096), ('游', 2094), ('游낕', 2080), ('游낗', 1218), ('游녢', 1108), ('游눜', 1040)]

3. **q3**:
   - [('@narendramodi', 2261), ('@Kisanektamorcha', 1836), ('@RakeshTikaitBKU', 1639), ('@PMOIndia', 1422), ('@RahulGandhi', 1125), ('@GretaThunberg', 1046), ('@RaviSinghKA', 1015), ('@rihanna', 972), ('@UNHumanRights', 962), ('@meenaharris', 925)]


# Gu칤a Paso a Paso del Proceso de Orquestaci칩n

## Pipeline de Orquestaci칩n en Azure Data Factory

### Descripci칩n General

Este proceso de orquestaci칩n est치 dise침ado para gestionar la ingesta, el procesamiento y la transformaci칩n de datos de tweets almacenados en Azure Data Lake Storage (ADLS). La orquestaci칩n se divide en dos pipelines principales: `Raw_ingestion` para la ingesta de datos en bruto y `Refined_ingestion` para el procesamiento y an치lisis de datos refinados.

### Pasos del Proceso

1. **Pipeline de Orquestaci칩n Principal**

    El pipeline principal, llamado `Orchestration`, orquesta la ejecuci칩n de dos pipelines subordinados:
    
    - **Bronze**: Ejecuta el pipeline `Raw_ingestion`.
    - **Silver**: Ejecuta el pipeline `Refined_ingestion` despu칠s de la finalizaci칩n exitosa del pipeline `Raw_ingestion`.

2. **Pipeline de Ingesta Bruta (Raw_ingestion)**

    Este pipeline se encarga de leer los datos en formato ZIP desde Azure Data Lake, descomprimirlos y almacenarlos en formato JSON en una ubicaci칩n espec칤fica en el Data Lake.
    
    - **Actividad de Ingesta de Datos**: Se configura una actividad de copia que lee los archivos ZIP desde el Data Lake, los descomprime y los almacena en formato JSON. La configuraci칩n incluye detalles sobre la fuente (Azure Blob Storage), el destino (Azure Blob Storage) y las configuraciones de formato de archivo.

3. **Pipeline de Ingesta Refinada (Refined_ingestion)**

    Este pipeline se encarga de ejecutar una serie de notebooks de Databricks que realizan diferentes c치lculos y transformaciones en los datos. Cada notebook est치 dise침ado para un prop칩sito espec칤fico de procesamiento y an치lisis.

    #### Actividades en `Refined_ingestion`:

    - **q1_memory**: 
        - Calcula las 10 fechas con la mayor cantidad de tweets.
        - Identifica el usuario con m치s tweets en cada una de esas fechas.
        - Optimiza el uso de memoria durante el procesamiento.
    
    - **q1_time**:
        - Calcula las 10 fechas con la mayor cantidad de tweets.
        - Identifica el usuario con m치s tweets en cada una de esas fechas.
        - Optimiza el tiempo de ejecuci칩n del proceso.
    
    - **q2_memory**:
        - Calcula los 10 emojis m치s usados y su conteo.
        - Optimiza el uso de memoria durante el procesamiento.
    
    - **q2_time**:
        - Calcula los 10 emojis m치s usados y su conteo.
        - Optimiza el tiempo de ejecuci칩n del proceso.
    
    - **q3_memory**:
        - Calcula los 10 usuarios m치s influyentes en funci칩n del conteo de menciones.
        - Optimiza el uso de memoria durante el procesamiento.
    
    - **q3_time**:
        - Calcula los 10 usuarios m치s influyentes en funci칩n del conteo de menciones.
        - Optimiza el tiempo de ejecuci칩n del proceso.

    Cada una de estas actividades est치 configurada para ejecutarse de manera secuencial o en paralelo, dependiendo de las dependencias definidas. Cada notebook recibe como par치metro la ruta del archivo JSON en el Data Lake y ejecuta sus respectivas tareas de procesamiento.



## Posibles Optimizaciones Futuras

- **Optimizaci칩n de Particiones**: Ajustar el n칰mero de particiones basadas en el tama침o del dataset para mejorar el paralelismo y el rendimiento.
- **Uso de `broadcast` para DataFrames Peque침os**: Utilizar la funci칩n `broadcast` de Spark para DataFrames peque침os que se unen frecuentemente, para optimizar el tiempo de uni칩n.
- **Manejo Avanzado de Errores**: Implementar estrategias de reintento y manejo de errores m치s detalladas para mejorar la robustez del proceso.
- **Escalado Autom치tico**: Configurar el escalado autom치tico del cl칰ster de Databricks para manejar din치micamente cargas de trabajo variables y optimizar el costo.
- **Profiling de C칩digo**: Realizar un profiling detallado del c칩digo utilizando herramientas como `PySpark Profiler` para identificar y optimizar partes espec칤ficas del c칩digo que consumen m치s recursos.
- **Ejecuci칩n autom치tica**: Una vez se defina la periodicidad de lectura y procesamiento de este ejercicio en un entorno real, puede agendarse mediante un trigger diario o mensual.
- **Logs avanzados**: Los logs pueden mejorarse para incluir m치s informaci칩n y mantener un registro en timestamp o por d칤a, definiendo correctamente el tama침o y coste por almacenamiento.

---

<p align="center" style="color: #A01F30;">
  Proyecto realizado por Juan Jos칠 Afanador para LATAM
</p>
